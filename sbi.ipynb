{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585e96b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===== IMPORTS & CONFIGURATION =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===== DATA INGESTION =====\n",
    "def load_data(file_path):\n",
    "    \"\"\"Simulate data loading with synthetic data\"\"\"\n",
    "    # Create synthetic data based on PDF specifications\n",
    "    n_samples = 327741\n",
    "    n_features = 139\n",
    "    fraud_ratio = 0.108\n",
    "    \n",
    "    # Create synthetic features based on PDF column names\n",
    "    data = pd.DataFrame(np.random.randn(n_samples, n_features), \n",
    "                       columns=[f'feature_{i}' for i in range(n_features)])\n",
    "    \n",
    "    # Add known columns from PDF\n",
    "    known_columns = [\n",
    "        'ACT_AGE', 'LIMIT', 'OUTS', 'ACT_INESTRUM', 'TERURE', 'LOIN', \n",
    "        'INSTALANT', 'ST_FLG', 'AGE', 'VENTAGE', 'KYC_SCR', 'CREDIT_HISTORY_LENGTH',\n",
    "        'NO_OF_INQUIRIES', 'TACQUE_BAND', 'AGREG_GROUP', 'SI_FLG', 'LOCKER_HLDR_IND',\n",
    "        'UID_FLG', 'KYC_FLG', 'INB_FLG', 'EKYCFLG', 'LATEST_CR_DAYS', 'ALL_LON_MAX_IRAC',\n",
    "        'LOAN_TENURE', 'LAST_1_YR_R64', 'LAST_3_YR_R64', 'CHST_NO_OF_TIMES_NPA',\n",
    "        'FIRST_NPA_TENURE', 'LATEST_NPA_TENURE', 'NO_YRS_NPA', 'NO_ENG', 'CRIFT_33',\n",
    "        'CRIFT_44', 'CRIFT_22', 'SIXAMTHAYGOTTO', 'SIXAMTHISCR', 'SIXAMTHISDR',\n",
    "        'SIXAMTHOUTSTANGBAL', 'SIXAMTHAYGMTD', 'FIVEMATHISCR', 'FIVEMATHAYGMTD',\n",
    "        'FIVEMATHOUTSTANGBAL', 'FIVEMATHISDR'\n",
    "    ]\n",
    "    \n",
    "    for col in known_columns:\n",
    "        if col not in data.columns:\n",
    "            data[col] = np.random.rand(n_samples)\n",
    "    \n",
    "    # Add target variable\n",
    "    fraud_cases = int(n_samples * fraud_ratio)\n",
    "    data['FRAUD'] = [1] * fraud_cases + [0] * (n_samples - fraud_cases)\n",
    "    np.random.shuffle(data['FRAUD'].values)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ===== DATA CLEANING =====\n",
    "def clean_data(data):\n",
    "    \"\"\"Clean data based on PDF specifications\"\"\"\n",
    "    # Drop high-missing columns (>50%)\n",
    "    missing_threshold = len(data) * 0.5\n",
    "    missing_counts = data.isnull().sum()\n",
    "    cols_to_drop = missing_counts[missing_counts > missing_threshold].index.tolist()\n",
    "    \n",
    "    # Add known columns to drop from PDF\n",
    "    cols_to_drop += ['LAST_1_YR_R64', 'LAST_3_YR_R64', 'CHST_NO_OF_TIMES_NPA',\n",
    "                    'FIRST_NPA_TENURE', 'LATEST_NPA_TENURE', 'NO_YRS_NPA', 'NO_ENG']\n",
    "    \n",
    "    # Remove duplicates\n",
    "    cols_to_drop = list(set(cols_to_drop))\n",
    "    data_cleaned = data.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Drop redundant columns (as per PDF)\n",
    "    data_cleaned = data_cleaned.drop(columns=['AGREG_GROUP', 'Unique ID'], errors='ignore')\n",
    "    \n",
    "    return data_cleaned\n",
    "\n",
    "# ===== FEATURE ENGINEERING =====\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.flag_columns = ['SI_FLG', 'LOCKER_HLDR_IND', 'UID_FLG', 'KYC_FLG', 'INB_FLG', 'EKYC_FLG']\n",
    "        self.balance_columns = [f'BALANCE_MONTH_{i}' for i in range(1, 13)]\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Create TOTAL_FLAGS feature\n",
    "        X['TOTAL_FLAGS'] = X[self.flag_columns].sum(axis=1)\n",
    "        \n",
    "        # Create split-averaged balances\n",
    "        if all(col in X.columns for col in self.balance_columns):\n",
    "            X['avg_balance_early'] = X[self.balance_columns[:5]].mean(axis=1)\n",
    "            X['avg_balance_late'] = X[self.balance_columns[5:]].mean(axis=1)\n",
    "        \n",
    "        # Create LATEST_CR_DAYS feature\n",
    "        if 'LATEST_CR_DAYS' in X.columns:\n",
    "            X['high_latest_cr_days'] = X['LATEST_CR_DAYS'].apply(\n",
    "                lambda x: 1 if 200 < x < 500 else 0)\n",
    "        \n",
    "        # Create ALL_LON_MAX_IRAC feature\n",
    "        if 'ALL_LON_MAX_IRAC' in X.columns:\n",
    "            X['non_irac_3_flag'] = X['ALL_LON_MAX_IRAC'].apply(\n",
    "                lambda x: 1 if x != 3 else 0)\n",
    "        \n",
    "        # Create LOAN_TENURE feature\n",
    "        if 'LOAN_TENURE' in X.columns:\n",
    "            X['short_loan_tenure_flag'] = X['LOAN_TENURE'].apply(\n",
    "                lambda x: 1 if x <= 1096 else 0)\n",
    "        \n",
    "        # Time-series feature aggregation\n",
    "        ts_features = ['SCR', 'SDR', 'OUTSTANGBAL', 'AVGMTD', 'AVGQTD', 'AVGYTD']\n",
    "        for feature in ts_features:\n",
    "            cols = [f'{feature}_MONTH_{i}' for i in range(1, 13)]\n",
    "            if all(col in X.columns for col in cols):\n",
    "                ts_data = X[cols]\n",
    "                X[f'{feature}_MEAN'] = ts_data.mean(axis=1)\n",
    "                X[f'{feature}_MIN'] = ts_data.min(axis=1)\n",
    "                X[f'{feature}_MAX'] = ts_data.max(axis=1)\n",
    "                X[f'{feature}_STD'] = ts_data.std(axis=1)\n",
    "                X[f'{feature}_FIRST'] = ts_data.iloc[:, 0]\n",
    "                X[f'{feature}_LAST'] = ts_data.iloc[:, -1]\n",
    "                X[f'{feature}_DIFF'] = X[f'{feature}_LAST'] - X[f'{feature}_FIRST']\n",
    "                \n",
    "                # Calculate slope using linear regression\n",
    "                for idx, row in ts_data.iterrows():\n",
    "                    slope = np.polyfit(range(12), row.values, 1)[0]\n",
    "                    X.at[idx, f'{feature}_SLOPE'] = slope\n",
    "        \n",
    "        # Convert account age and credit history to months\n",
    "        if 'ACCOUNT_AGE' in X.columns:\n",
    "            X['ACCOUNT_AGE_MONTHS'] = X['ACCOUNT_AGE'] * 12\n",
    "            \n",
    "        if 'CREDIT_HISTORY_LENGTH' in X.columns:\n",
    "            # Assuming format \"Xyrs & Ymon\"\n",
    "            X['CREDIT_HISTORY_MONTHS'] = X['CREDIT_HISTORY_LENGTH'].apply(\n",
    "                lambda s: int(s.split('yrs')[0])*12 + int(s.split('&')[1].split('mon')[0]))\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ===== STAGE 1: HIGH-RECALL FILTERING =====\n",
    "def stage1_filtering(X, y, threshold=0.3):\n",
    "    \"\"\"Filter dataset using high-recall XGBoost model\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    xgb = XGBClassifier(scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "                        eval_metric='logloss',\n",
    "                        random_state=42)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probabilities and apply threshold\n",
    "    y_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    recall = tp / (tp + fn)\n",
    "    fraud_miss_rate = fn / (tp + fn)\n",
    "    non_fraud_accuracy = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"Stage 1 Performance (Threshold={threshold}):\")\n",
    "    print(f\"Recall: {recall:.4f}, Fraud Miss Rate: {fraud_miss_rate:.4f}\")\n",
    "    print(f\"Non-Fraud Accuracy: {non_fraud_accuracy:.4f}\")\n",
    "    print(f\"Filtered Subset Size: {tp + fp} ({((tp+fp)/len(X_test))*100:.2f}% of test set)\")\n",
    "    \n",
    "    # Filter training data for Stage 2\n",
    "    train_proba = xgb.predict_proba(X_train)[:, 1]\n",
    "    train_filtered_idx = np.where(train_proba > threshold)[0]\n",
    "    \n",
    "    return X_train.iloc[train_filtered_idx], y_train.iloc[train_filtered_idx], xgb\n",
    "\n",
    "# ===== STAGE 2: NEURAL NETWORK =====\n",
    "def build_nn_model(input_dim):\n",
    "    \"\"\"Create neural network architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_thresholds(y_true, y_proba):\n",
    "    \"\"\"Evaluate model at different thresholds\"\"\"\n",
    "    results = []\n",
    "    for threshold in [0.5, 0.6, 0.7, 0.8]:\n",
    "        y_pred = (y_proba > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ===== ENSEMBLE WITH UNCERTAINTY HANDLING =====\n",
    "def ensemble_voting(X_train, y_train, X_test):\n",
    "    \"\"\"Ensemble model with uncertainty handling\"\"\"\n",
    "    models = [\n",
    "        ('xgb', XGBClassifier(random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100,), random_state=42)),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n",
    "        ('logreg', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('extratrees', ExtraTreesClassifier(random_state=42)),\n",
    "        ('randomforest', RandomForestClassifier(random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Train individual models\n",
    "    trained_models = {}\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "    \n",
    "    # Generate predictions with uncertainty\n",
    "    all_preds = []\n",
    "    for name, model in trained_models.items():\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        all_preds.append(y_proba)\n",
    "    \n",
    "    # Apply dynamic thresholding\n",
    "    final_preds = np.zeros(X_test.shape[0])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        model_preds = [preds[i] for preds in all_preds]\n",
    "        sorted_preds = np.sort(model_preds)\n",
    "        \n",
    "        # Classify based on percentiles\n",
    "        if sorted_preds[4] >= np.percentile(model_preds, 80):  # Top 20% (5th model in sorted list)\n",
    "            final_preds[i] = 1  # Fraud\n",
    "        elif sorted_preds[2] <= np.percentile(model_preds, 20):  # Bottom 20% (3rd model)\n",
    "            final_preds[i] = 0  # Non-fraud\n",
    "        else:\n",
    "            final_preds[i] = 2  # Uncertain\n",
    "    \n",
    "    return final_preds, trained_models\n",
    "\n",
    "# ===== MAIN EXECUTION PIPELINE =====\n",
    "def main():\n",
    "    # Step 1: Data Ingestion\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(\"PSB_Hackathon_data.csv\")\n",
    "    \n",
    "    # Step 2: Data Cleaning\n",
    "    print(\"Cleaning data...\")\n",
    "    data_cleaned = clean_data(data)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data_cleaned.drop(columns=['FRAUD'])\n",
    "    y = data_cleaned['FRAUD']\n",
    "    \n",
    "    # Step 3: Feature Engineering\n",
    "    print(\"Engineering features...\")\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    X_engineered = feature_engineer.fit_transform(X)\n",
    "    \n",
    "    # Step 4: Preprocessing\n",
    "    # Identify categorical and numerical columns\n",
    "    cat_cols = X_engineered.select_dtypes(include=['object', 'category']).columns\n",
    "    num_cols = X_engineered.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "    \n",
    "    X_preprocessed = preprocessor.fit_transform(X_engineered)\n",
    "    \n",
    "    # Step 5: Stage 1 - High-Recall Filtering\n",
    "    print(\"Stage 1: High-recall filtering...\")\n",
    "    X_filtered, y_filtered, stage1_model = stage1_filtering(\n",
    "        X_preprocessed, y, threshold=0.3)\n",
    "    \n",
    "    # Save Stage 1 model\n",
    "    joblib.dump(stage1_model, 'stage1_xgb_model.pkl')\n",
    "    \n",
    "    # Step 6: Stage 2 - Neural Network\n",
    "    print(\"\\nStage 2: Neural Network Training...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "    \n",
    "    # Add XGBoost probabilities as feature\n",
    "    xgb_proba = stage1_model.predict_proba(X_train)[:, 1]\n",
    "    X_train = np.hstack([X_train, xgb_proba.reshape(-1, 1)])\n",
    "    \n",
    "    # Build and train NN\n",
    "    nn_model = build_nn_model(X_train.shape[1])\n",
    "    history = nn_model.fit(X_train, y_train, \n",
    "                          epochs=50, \n",
    "                          batch_size=256, \n",
    "                          validation_split=0.2,\n",
    "                          verbose=1)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_xgb_proba = stage1_model.predict_proba(X_test)[:, 1]\n",
    "    X_test_nn = np.hstack([X_test, test_xgb_proba.reshape(-1, 1)])\n",
    "    y_proba_nn = nn_model.predict(X_test_nn)\n",
    "    \n",
    "    # Evaluate at different thresholds\n",
    "    threshold_results = evaluate_thresholds(y_test, y_proba_nn)\n",
    "    print(\"\\nNeural Network Threshold Performance:\")\n",
    "    print(threshold_results)\n",
    "    \n",
    "    # Save NN model\n",
    "    nn_model.save('stage2_nn_model.h5')\n",
    "    \n",
    "    # Step 7: Ensemble with Uncertainty Handling\n",
    "    print(\"\\nTraining Ensemble with Uncertainty Handling...\")\n",
    "    # Use filtered data without XGB proba feature\n",
    "    X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(\n",
    "        X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "    \n",
    "    ensemble_preds, ensemble_models = ensemble_voting(\n",
    "        X_train_ensemble, y_train_ensemble, X_test_ensemble)\n",
    "    \n",
    "    # Analyze results\n",
    "    confident_preds = ensemble_preds[ensemble_preds != 2]\n",
    "    confident_true = y_test_ensemble[ensemble_preds != 2]\n",
    "    uncertain_count = np.sum(ensemble_preds == 2)\n",
    "    \n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"Confident predictions: {len(confident_preds)}\")\n",
    "    print(f\"Uncertain cases: {uncertain_count} ({uncertain_count/len(ensemble_preds)*100:.2f}%)\")\n",
    "    print(\"\\nConfident Prediction Performance:\")\n",
    "    print(classification_report(confident_true, confident_preds))\n",
    "    \n",
    "    # Save ensemble models\n",
    "    joblib.dump(ensemble_models, 'ensemble_models.pkl')\n",
    "    \n",
    "    print(\"\\nPipeline execution complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
